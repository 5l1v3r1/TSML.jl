var documenterSearchIndex = {"docs":
[{"location":"#","page":"HOME","title":"HOME","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"#TSML-(Time-Series-Machine-Learning)-1","page":"HOME","title":"TSML (Time-Series Machine Learning)","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"TSML (Time Series Machine Learning) is package  for Time Series data processing, classification, and prediction. It combines ML libraries from Python's  ScikitLearn, R's Caret, and Julia ML using a common API  and allows seamless ensembling and integration of  heterogenous ML libraries to create complex models  for robust time-series pre-processing and prediction/classification.","category":"page"},{"location":"#Package-Features-1","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"TS aggregation based on time/date interval\nTS imputation based on Nearest Neighbors\nTS statistical metrics of data quality\nTS classification for automatic data discovery\nTS prediction with more than 100+ libraries from caret, scikitlearn, and julia\nTS date/val matrix conversion of 1-d TS using sliding windows for ML input\nPipeline API allows high-level description of the processing workflow\nEasily extensible architecture by using just two main interfaces: fit and transform\nSupport for hundreds of external ML libs from Scikitlearn and Caret by using common API wrappers for PyCall and RCall","category":"page"},{"location":"#Installation-1","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"TSML is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add TSML","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add TSML\"","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"TSML\")","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> pkg\"add TSML\"","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Once TSML is installed, you can load the TSML package by:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using TSML","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> import TSML","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Generally, you will need the different transformers and utils in TSML for time-series processing. To use them, it is standard in TSML code to have the following declared at the topmost part of your application:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"using TSML \nusing TSML.TSMLTransformers\nusing TSML.TSMLTypes\nusing TSML.Utils","category":"page"},{"location":"#Tutorial-Outline-1","page":"HOME","title":"Tutorial Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"tutorial/aggregators.md\",\n  \"tutorial/pipeline.md\",\n  \"tutorial/statistics.md\",\n  \"tutorial/monotonic.md\",\n  \"tutorial/tsclassifier.md\"\n]\nDepth = 3","category":"page"},{"location":"#Manual-Outline-1","page":"HOME","title":"Manual Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"man/valueproc.md\",\n  \"man/dateproc.md\",\n  \"man/aggregation.md\",\n  \"man/imputation.md\",\n]\nDepth = 3","category":"page"},{"location":"#ML-Library-1","page":"HOME","title":"ML Library","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\"lib/decisiontree.md\"]","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/aggregators/#aggregators_imputers-1","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"","category":"section"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The package assumes a two-column table composed of Dates and Values.  The first part of the workflow aggregates values based on the specified  date-time interval which minimizes occurence of missing values and noise.  The aggregated data is then left-joined to the complete sequence of  DateTime  in a specified date-time interval. Remaining missing values are replaced  by k nearest neighbors where k is the symmetric distance from the location  of missing value. This replacement algo is called several times until there  are no more missing values.","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let us create a Date, Value table with some missing values and output the first 15 rows. We will then apply some TSML functions to normalize/clean the data. Below is the code of the generateDataWithMissing() function:","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend\n\nX = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/aggregators/#DateValgator-1","page":"Aggregators and Imputers","title":"DateValgator","text":"","category":"section"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"You'll notice several blocks of missing in the table above with reading frequency of every 15 minutes.  To minimize noise and lessen the occurrence of missing values, let's aggregate our dataset by taking the hourly median using the DateValgator transformer.","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\nusing TSML.TSMLTypes\nusing TSML.Utils\nusing TSML.TSMLTransformers\nusing TSML: DateValgator\n\ndtvlgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(dtvlgator,X)\nresults = transform!(dtvlgator,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The occurrence of missing values is now reduced because of the hourly aggregation. While the default is hourly aggregation, you can easily change it by using a different interval in the argument during instance creation. Below indicates every 30 minutes interval.","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"dtvlgator = DateValgator(Dict(:dateinterval=>Dates.Minute(30)))","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"DateValgator is one of the several TSML transformers to preprocess and clean the  time series data. In order to create additional transformers to extend TSML,  each transformer must overload the two Transformer functions:fit! and transform!.  DateValgator fit! performs initial setups of necessary parameters and validation of arguments while its transform! function contains the algorithm  for aggregation. ","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"For machine learning prediction and classification transformer,  fit! function is equivalent to ML training or parameter optimization,  while the transform! function is for doing the actual prediction. The later part of the tutorial will provide an example how to add a Transformer to extend the functionality of TSML.","category":"page"},{"location":"tutorial/aggregators/#DateValNNer-1","page":"Aggregators and Imputers","title":"DateValNNer","text":"","category":"section"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let's perform further processing to replace the remaining missing values with their nearest neighbors.  We will use DateValNNer which is a TSML transformer to process the output of DateValgator. DateValNNer can also process non-aggregated data by first running similar workflow of DateValgator before performing its imputation routine.","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML: DateValNNer\n\ndatevalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(datevalnner, X)\nresults = transform!(datevalnner,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"After running the DateValNNer, it's guaranteed that there will be no more missing data unless the input are all missing data.","category":"page"},{"location":"tutorial/aggregators/#DateValizer-1","page":"Aggregators and Imputers","title":"DateValizer","text":"","category":"section"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"One more imputer to replace missing data is DateValizer. It computes the hourly median over 24 hours and use the hour => median hashmap learned to replace missing data using hour as the key. In this implementation, fit! function is doing the training of parameters by computing the medians and save it for the transform! function to use for imputation. It is possible that the hashmap can contain missing values in cases where the pooled hourly median in a particular hour have all missing data. Below is a sample workflow to replace missing data in X with the hourly medians.","category":"page"},{"location":"tutorial/aggregators/#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML: DateValizer\n\ndatevalizer = DateValizer(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(datevalizer, X)\nresults = transform!(datevalizer,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/pipeline/#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Instead of calling fit! and transform! for each transformer to process time series data, we can use the Pipeline transformer which does this automatically by iterating through the transformers and calling fit! and transform! repeatedly for each transformer in its argument.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let's start again by using a function to generate a time series dataframe with some missing data.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"X = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/pipeline/#Workflow-of-Pipeline-1","page":"Pipeline","title":"Workflow of Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let's use the pipeline transformer to aggregate and impute:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using Dates\nusing TSML\nusing TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Pipeline\nusing TSML: DateValgator\nusing TSML: DateValNNer\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\n\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalnner\n         ]\n  )\n)\n\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Using the Pipeline transformer, it becomes straightforward to process the time series data. It also becomes trivial to extend TSML functionality by adding more transformers and making sure each support the fit! and transform! interfaces. Any new transformer can then be easily added to the Pipeline workflow  without invasively changing the existing codes.","category":"page"},{"location":"tutorial/pipeline/#Extending-TSML-1","page":"Pipeline","title":"Extending TSML","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"To illustrate how simple it is to add a new transformer, below extends TSML by adding CSVReader transformer and added in the pipeline to process CSV data:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using TSML.TSMLTypes\nusing TSML.Utils\nimport TSML.TSMLTypes.fit!\nimport TSML.TSMLTypes.transform!\n\nusing CSV\n\nmutable struct CSVReader <: Transformer\n    model\n    args\n    function CSVReader(args=Dict())\n        default_args = Dict(\n            :filename => \"\",\n            :dateformat => \"\"\n        )\n        new(nothing,mergedict(default_args,args))\n    end\nend\n\nfunction fit!(csvrdr::CSVReader,x::T=[],y::Vector=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n    fname = csvrdr.args[:filename]\n    fmt = csvrdr.args[:dateformat]\n    (fname != \"\" && fmt != \"\") || error(\"missing filename or date format\")\n    model = csvrdr.args\nend\n\nfunction transform!(csvrdr::CSVReader,x::T=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n    fname = csvrdr.args[:filename]\n    fmt = csvrdr.args[:dateformat]\n    df = CSV.read(fname)\n    ncol(df) == 2 || error(\"dataframe should have only two columns: Date,Value\")\n    rename!(df,names(df)[1]=>:Date,names(df)[2]=>:Value)\n    df[:Date] = DateTime.(df[:Date],fmt)\n    df\nend","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Instead of passing table X that contains the time series, we will add  an instance of theCSVReader at the start of the array of transformers in the pipeline  to read the csv data. CSVReader transform! function converts the csv time series table into a dataframe, which will be consumed by the next transformer in the pipeline  for processing.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"fname = joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvreader = CSVDateValReader(Dict(:filename=>fname,:dateformat=>\"d/m/y H:M\"))\nfit!(csvreader)\ncsvdata = transform!(csvreader)\nfirst(csvdata,10)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us now include the newly created CSVReader in the pipeline to read the csv data and process it by aggregation and imputation.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"mypipeline = Pipeline(\n  Dict( :transformers => [\n            csvreader,\n            dtvalgator,\n            dtvalnner\n         ]\n  )\n)\n\nfit!(mypipeline)\nresults = transform!(mypipeline)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Notice that there is no more the need to pass X in the arguments of fit! and transform because the data is now transmitted by the CSVReader instance to the other transformers in the pipeline.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/statistics/#Statistical-Metrics-1","page":"Statistical Metrics","title":"Statistical Metrics","text":"","category":"section"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Each TS can be evaluated to extract its statistical features which can be used for data quality assessment, data discovery by clustering and classification, and anomaly characterization among others.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"TSML relies on Statifier to perform statistical metrics on the TS which can be configured to extract the statistics of missing blocks aside from the non-missing elements. Some of the scalar statistics it uses include: pacf, acf, autocor, quartiles, mean, median, max, min, kurtosis, skewness, variation, standard error, entropy, etc. It has only one argument :processmissing => true which indicates whether to include the statistics of missing data.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us again start generating an artificial data with missing values  using the generateDataWithMissing() described in the beginning of tutorial.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"X = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/statistics/#Statifier-for-Both-Non-Missing-and-Missing-Values-1","page":"Statistical Metrics","title":"Statifier for Both Non-Missing and Missing Values","text":"","category":"section"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"TSML includes Statifier transformer that computes scalar statistics to characterize the time series data. By default, it also computes statistics of  missing blocks of data. To disable this feature, one can pass  :processmissing => false to the argument during its instance creation. Below illustrates this workflow.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"using Dates\nusing TSML\nusing TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Pipeline\nusing TSML: DateValgator\nusing TSML: DateValNNer\nusing TSML: Statifier\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\ndtvalizer = DateValizer(Dict(:dateinterval => Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing => true))\n\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            stfier\n         ]\n  )\n)\n\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics/#Statifier-for-Non-Missing-Values-only-1","page":"Statistical Metrics","title":"Statifier for Non-Missing Values only","text":"","category":"section"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"If you are not intested with the statistics of the missing blocks, you can disable missing blocks stat summary by indicating :processmissing => false in the instance argument:","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>false))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics/#Statifier-After-Imputation-1","page":"Statistical Metrics","title":"Statifier After Imputation","text":"","category":"section"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us check the statistics after the imputation by adding DateValNNer instance in the pipeline. We expect that if the imputation is successful, the stats for missing blocks will all be NaN because stats of empty set is an NaN.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalnner,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"As we expected, the imputation is successful and there are no more missing values in the processed time series dataset.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let's try with the other imputation using DateValizer and validate that there are no more missing values based on the stats.","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalizer,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics/#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Indeed, the imputation got rid of the missing values.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/monotonic_plotting/#Monotonic-Detection-and-Plotting-1","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"One important preprocessing step for time series data processing is the detection  of monotonic data and transform it to non-monotonic type by using the finite difference operator.","category":"page"},{"location":"tutorial/monotonic_plotting/#Artificial-Data-Example-1","page":"Monotonic Detection and Plotting","title":"Artificial Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's create an artificial monotonic data and apply our monotonic transformer to normalize it. We can use the Plotter filter to visualize the generated data.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using Dates, DataFrames, Random\nusing TSML, TSML.Utils, TSML.TSMLTypes\nusing TSML: Plotter\n\nRandom.seed!(123)\npltr = Plotter(Dict(:interactive => false))\nmdates = DateTime(2017,12,1,1):Dates.Hour(1):DateTime(2017,12,31,10) |> collect\nmvals = rand(length(mdates)) |> cumsum\ndf =  DataFrame(Date=mdates ,Value = mvals)\nfit!(pltr,df)\ntransform!(pltr,df)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Now that we have a monotonic data, let's use the Monotonicer to normalize and plot the result:","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML, TSML.Utils, TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Monotonicer\n\nmono = Monotonicer(Dict())\n\npipeline = Pipeline(Dict(\n   :transformers => [mono,pltr]\n   )\n)\n\nfit!(pipeline,df)\nres=transform!(pipeline,df)\n","category":"page"},{"location":"tutorial/monotonic_plotting/#Real-Data-Example-1","page":"Monotonic Detection and Plotting","title":"Real Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We will now apply the entire pipeline  starting from reading csv data, aggregate, impute, normalize if it's monotonic, and plot. We will consider three  different data types: a regular time series data, a   monotonic data, and a daily monotonic data. The difference between   monotonic and daily monotonic is that the values in daily monotonic resets to  zero or some baseline and cumulatively increases in a day until the  next day where it resets to zero or some baseline value. Monotonicer automatically detects these three different types and apply the corresponding normalization accordingly.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML: DateValgator, DateValNNer, Statifier, Monotonicer\nregularfile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/regular.csv\")\nmonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/monotonic.csv\")\ndailymonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/dailymonotonic.csv\")\n\nregularfilecsv = CSVDateValReader(Dict(:filename=>regularfile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nmonofilecsv = CSVDateValReader(Dict(:filename=>monofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\ndailymonofilecsv = CSVDateValReader(Dict(:filename=>dailymonofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\n\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\npltr = Plotter(Dict(:interactive => false))\nnothing #hide","category":"page"},{"location":"tutorial/monotonic_plotting/#Regular-TS-Processing-1","page":"Monotonic Detection and Plotting","title":"Regular TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's test by feeding the regular time series type to the pipeline. We expect that for this type, Monotonicer will not perform further processing:","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [regularfilecsv,valgator,valnner,mono,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [regularfilecsv,valgator,valnner,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that the plots are the same with or without the Monotonicer instance.","category":"page"},{"location":"tutorial/monotonic_plotting/#Monotonic-TS-Processing-1","page":"Monotonic Detection and Plotting","title":"Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's now feed the same pipeline with a monotonic csv data.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [monofilecsv,valgator,valnner,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [monofilecsv,valgator,valnner,mono,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that without the Monotonicer instance, the data is monotonic. Applying the Monotonicer instance in the pipeline converts the data into a regular time series but with outliers.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We can use the Outliernicer filter to remove outliers. Let's apply this filter after the Monotonicer and plot the result.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML: Outliernicer\noutliernicer = Outliernicer(Dict(:dateinterval=>Dates.Hour(1)));\n\npipeline = Pipeline(Dict(\n    :transformers => [monofilecsv,valgator,valnner,mono, outliernicer,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#Daily-Monotonic-TS-Processing-1","page":"Monotonic Detection and Plotting","title":"Daily Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Lastly, let's feed the daily monotonic data using similar pipeline and examine its plot.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [dailymonofilecsv,valgator,valnner,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"This plot is characterized by monotonically increasing trend but resets to certain baseline value  at the end of the day and repeat similar trend daily. The challenge for the monotonic normalizer is to differentiate between daily monotonic from the typical monotonic function to apply the correct normalization.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [dailymonofilecsv,valgator,valnner,mono,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"While the Monotonicer filter is able to transform the data into a regular time series, there are significant outliers due to noise and the nature of this kind of data or sensor.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's remove the outliers by applying the Outliernicer filter and examine the result.","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = Pipeline(Dict(\n    :transformers => [dailymonofilecsv,valgator,valnner,mono,outliernicer,pltr]\n   )\n)\nfit!(pipeline)\ntransform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"The Outliernicer filter effectively removed the outliers as shown in the plot.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/tsclassifier/#TS-Data-Discovery-1","page":"TS Data Discovery","title":"TS Data Discovery","text":"","category":"section"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"We have enough building blocks to perform data discovery given a bunch  of time series data generated by sensors. Processing hundreds or thousands of time series data is becoming a common occurrence and typical challenge nowadays with the rapid adoption of IoT technology in buildings, manufacturing industries, etc.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this section, we will use those transformers discussed in the previous sections to normalize and extract the statistical features of TS. These extracted stat features will be used as input to a Machine learning model. We will train this model to learn the signatures of different TS types so that we can use it to classify unknown or unlabeled sensor data.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this tutorial, we will use TSClassifier which works in the following context:  Given a bunch of time-series with specific types. Get the statistical features of each, use these as inputs to a classifier with output as the TS type, train, and test. Another option is to use these stat features for clustering and check cluster quality. If accuracy is poor, add more stat features and repeat same process as outlined for training and testing. Assume that each time series during training is named based on their type which will be used as the target output. For example, temperature time series will be named as temperature?.csv where ? is any positive integer. Using this setup, the TSClassifier loops over each file in the training directory, get the stats and record these accumulated stat features into a dataframe and train the model to learn the input->output mapping during fit! operation. Apply the learned models in the transform! operation loading files in the testing directory.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The entire process of training to learn the appropriate parameters and classification to identify unlabeled data exploits the idea of the pipeline workflow discussed in the previous sections.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's illustrate the process by loading some sample data:","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using Random\nusing TSML\n\nRandom.seed!(12345)\n\ntrdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/training\")\ntstdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/testing\")\nmodeldirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/model\")","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Here's the list of files for training:","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(trdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"and here are the files in testing directory:","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(tstdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The files in testing directory doesn't need to be labeled but we use the labeling as a way to validate the effectiveness of the classifier. The labels will be used as the groundtruth during prediction/classification.","category":"page"},{"location":"tutorial/tsclassifier/#TSClassifier-1","page":"TS Data Discovery","title":"TSClassifier","text":"","category":"section"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let us now setup an instance of the TSClassifier and pass the arguments containing the directory locations of files for training, testing, and modeling.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using TSML: TSClassifier\nusing TSML: fit!, transform!\n\ntscl = TSClassifier(Dict(:trdirectory=>trdirname,\n          :tstdirectory=>tstdirname,\n          :modeldirectory=>modeldirname,\n          :feature_range => 6:20,\n          :num_trees=>20)\n       )\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Time to train our TSClassifier to learn the mapping between extracted stats features with the  TS type.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"fit!(tscl)","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"We can examine the extracted features saved by the model that is used for its training.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using CSV, DataFrames\n\nmdirname = tscl.args[:modeldirectory]\nmodelfname=tscl.args[:juliarfmodelname]\n\ntrstatfname = joinpath(mdirname,modelfname*\".csv\")\nres = CSV.read(trstatfname) |> DataFrame\nfirst(res,5)","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's check the accuracy of prediction with the test data using the transform! function.","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"dfresults = transform!(tscl)","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The table above shows the prediction corresponding to each filename which is the groundtruth. We can compute the accuracy by extracting from the filename the TS type and compare it with the corresponding prediction. Below computes the prediction accuracy:","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"prediction = dfresults[:predtype]\nfnames = dfresults[:fname]\nmyregex = r\"(?<dtype>[A-Z _ - a-z]+)(?<number>\\d*).(?<ext>\\w+)\"\ngroundtruth=map(fnames) do fname\n  mymatch=match(myregex,fname)\n  mymatch[:dtype]\nend\nsum(groundtruth .== prediction) / length(groundtruth) * 100","category":"page"},{"location":"tutorial/tsclassifier/#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Of course we need more data to split between training and testing to improve accuracy and get a more stable measurement of performance.","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/valueproc/#valueprep-1","page":"Value PreProcessing","title":"Value Preprocessing","text":"","category":"section"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"In order to process 1-D TS as input for ML model, it has to be converted into Matrix form where each row represents a  slice of 1-D TS representing daily/hourly/weekly pattern depending on the size of the chunk, stride, and  number of steps ahead for prediction. Below illustrates the processing workflow to Matrify a 1-D TS.","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"For illustration purposes, the code below generates a  Date,Value dataframe where the values are just a sequece of integer from 1 to the length of the date sequence. We use this simple sequence to have a better understanding how the slicing of rows, steps ahead, and the stride to create the Matrified output is generated.","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"using Dates\nusing TSML, TSML.Utils, TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing DataFrames\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2017,1,5)\ndat=lower:Dates.Hour(1):upper |> collect\nvals = 1:length(dat)\nx = DataFrame(Date=dat,Value=vals)\nlast(x,5)","category":"page"},{"location":"man/valueproc/#Matrifier-1","page":"Value PreProcessing","title":"Matrifier","text":"","category":"section"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"Let us create an instance of Matrifier passing the size of row, stride, and steps ahead to predict:","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"mtr = Matrifier(Dict(:ahead=>6,:size=>6,:stride=>3))\nfit!(mtr,x)\nres = transform!(mtr,x)\nfirst(res,5)","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"In this example, we have hourly values. We indicated in the  Matrifier to generate a matrix where the size of each row is 6 hours, steps ahead for prediction is 6 hours and the stride of 3 hours. There are 7 columns because the last column indicates the value indicated by the steps ahead argument.","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"Let us try to make a matrix with the size of 6 hours, steps ahead of 2 hours, and a stride of 3 hours:","category":"page"},{"location":"man/valueproc/#","page":"Value PreProcessing","title":"Value PreProcessing","text":"mtr = Matrifier(Dict(:ahead=>2,:size=>6,:stride=>3))\nfit!(mtr,x)\nres = transform!(mtr,x)\nfirst(res,5)","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/dateproc/#Date-Preprocessing-1","page":"Date PreProcessing","title":"Date Preprocessing","text":"","category":"section"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"Extracting the Date features in a Date,Value table follows similar workflow with the value preprocessing  of the previous section. The main difference  is we are only interested on the date corresponding to the last column of the values generated by the Matrifier. This last column contains the values before  the prediction happens and the dates corresponding to these values carry significant information based on recency compared to the other dates.","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"Let us start by creating a Date,Value dataframe similar to the previous section.","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"using Dates\nusing TSML, TSML.Utils, TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing DataFrames\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2018,1,31)\ndat=lower:Dates.Day(1):upper |> collect\nvals = rand(length(dat))\nx = DataFrame(Date=dat,Value=vals)\nfirst(x,5)","category":"page"},{"location":"man/dateproc/#Dateifier-1","page":"Date PreProcessing","title":"Dateifier","text":"","category":"section"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"Let us create an instance of Dateifier passing the size of row, stride, and steps ahead to predict:","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"mtr = Dateifier(Dict(:ahead=>24,:size=>24,:stride=>5))\nfit!(mtr,x)\nres = transform!(mtr,x)\nfirst(res,5)","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"The model transform! output extracts automatically several date features such as year, month, day, hour, week, day of the week,  day of quarter, quarter of year.","category":"page"},{"location":"man/dateproc/#ML-Features:-Matrifier-and-Datefier-1","page":"Date PreProcessing","title":"ML Features: Matrifier and Datefier","text":"","category":"section"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"You can then combine the outputs in both the Matrifier and Datefier  as input features to a machine learning model. Below is an example of the workflow where the code extracts the Date and Value features combining them to form a matrix of features as input to a machine learning model.","category":"page"},{"location":"man/dateproc/#","page":"Date PreProcessing","title":"Date PreProcessing","text":"commonargs = Dict(:ahead=>3,:size=>5,:stride=>2)\ndtr = Dateifier(commonargs)\nmtr = Matrifier(commonargs)\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2018,1,31)\ndat=lower:Dates.Day(1):upper |> collect\nvals = rand(length(dat))\nX = DataFrame(Date=dat,Value=vals)\n\nfit!(mtr,X)\nvaluematrix = transform!(mtr,X)\nfit!(dtr,X)\ndatematrix = transform!(dtr,X)\nmlfeatures = hcat(datematrix,valuematrix)\nfirst(mlfeatures,5)","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/aggregation/#Aggregation-1","page":"Aggregation","title":"Aggregation","text":"","category":"section"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"DateValgator is a data type that supports operation  for aggregation to minimize noise and  lessen the occurrence of missing data. It expects to receive one argument which is the date-time interval for grouping values by taking  their median. For example, hourly median as the basis of aggregation can be carried out by passing this argument: :dateinterval => Dates.Hour(1)","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"To illustrate DateValgator usage, let's start by  generating an artificial data with sample frequencey every 5 minutes and print the first 10 rows.","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"using Dates, DataFrames\ngdate = DateTime(2014,1,1):Dates.Minute(5):DateTime(2014,5,1)\ngval = rand(length(gdate))\n\ndf = DataFrame(Date=gdate,Value=gval)\nfirst(df,10)","category":"page"},{"location":"man/aggregation/#DateValgator-1","page":"Aggregation","title":"DateValgator","text":"","category":"section"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"Let's apply the aggregator and try diffent groupings: hourly vs half hourly vs daily aggregates of the data.","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"using TSML, TSML.TSMLTransformers, TSML.Utils, TSML.TSMLTypes\n\nhourlyagg = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\nhalfhourlyagg = DateValgator(Dict(:dateinterval => Dates.Minute(30)))\ndailyagg = DateValgator(Dict(:dateinterval => Dates.Day(1)))\n\nfit!(halfhourlyagg,df)\nhalfhourlyres = transform!(halfhourlyagg,df)\n\nfit!(hourlyagg,df)\nhourlyres = transform!(hourlyagg,df)\n\nfit!(dailyagg,df)\ndailyres = transform!(dailyagg,df)\nnothing #hide","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"The first 5 rows of half-hourly, hourly, and daily aggregates:","category":"page"},{"location":"man/aggregation/#","page":"Aggregation","title":"Aggregation","text":"first(halfhourlyres,5)\nfirst(hourlyres,5)\nfirst(dailyres,5)","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/imputation/#Imputation-1","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"There are two ways to impute the date,value TS data. One uses DateValNNer which uses nearest neighbor and DateValizer which uses the dictionary of medians mapped to  certain date-time interval grouping.","category":"page"},{"location":"man/imputation/#DateValNNer-1","page":"Imputation","title":"DateValNNer","text":"","category":"section"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"DateValNNer expects the following arguments with their default values during instantation: ","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":":dateinterval => Dates.Hour(1)  \ngrouping interval\n:nnsize => 1 \nsize of neighborhood\n:missdirection => :symmetric \n:forward vs :backward vs :symmetric\n:strict => true \nwhether or not to repeatedly iterate until no more missing data","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"The :missdirection indicates the imputation direction and the extent of neighborhood. Symmetric implies getting info from both sides of the missing data. :forward direction starts imputing from the top while the :reverse starts from the bottom. Please refer to  Aggregators and Imputers for other examples.","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"using Random\nusing TSML\nusing TSML.Utils\nusing TSML.TSMLTypes\nusing TSML.TSMLTransformers\n\nusing Dates, DataFrames\n\nfunction generateXY()\n    Random.seed!(123)\n    gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n    gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n    gmissing = 50000\n    gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n    X = DataFrame(Date=gdate,Value=gval)\n    X[:Value][gndxmissing] .= missing\n    Y = rand(length(gdate))\n    (X,Y)\nend\nX,Y = generateXY()","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Let's use the same dataset we have used in the tutorial and print the first few rows.","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"first(X,10)","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Let's try the following setup grouping daily with forward imputation and 10 neighbors:","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :forward,\n             :strict=>false))\nfit!(dnnr,X)\nforwardres=transform!(dnnr,X)\nfirst(forwardres,5)","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Same parameters as above but uses reverse instead of forward direction:","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :reverse,\n             :strict=>false))\nfit!(dnnr,X)\nreverseres=transform!(dnnr,X)\nfirst(reverseres,5)","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Using symmetric imputation:","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :symmetric,\n             :strict=>false))\nfit!(dnnr,X)\nsymmetricres=transform!(dnnr,X)\nfirst(symmetricres,5)","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Unlike symmetric imputation that guarantees 100% imputation of missing data as long as the input has non-missing elements, forward and reverse cannot guarantee that the imputation replaces all missing data because of the boundary issues. If the top or bottom of the input is missing, the assymetric imputation will not be able to replace the endpoints that are missing. It is advised that to have successful imputation, symmetric imputation shall be used.","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"In the example above, the number of remaining missing data not imputed for forward, reverse, and symmetric is:","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"sum(ismissing.(forwardres[:Value]))\nsum(ismissing.(reverseres[:Value]))\nsum(ismissing.(symmetricres[:Value]))","category":"page"},{"location":"man/imputation/#DateValizer-1","page":"Imputation","title":"DateValizer","text":"","category":"section"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"DateValizer operates on the principle that there is a reqularity of patterns in a specific time period such that replacing values is just a matter of  extracting which time period it belongs and used the pooled median in that time period to replace the missing data. The default time period for DateValizer is hourly. In a more advanced implementation, we can add daily, hourly, and weekly  periods but it will require much larger hash table. Additional grouping criteria  can result into smaller subgroups which may contain 100% missing in some of these subgroups resulting to imputation failure. DateValizer only depends on the :dateinterval => Dates.Hour(1)  argument with default value of hourly. Please refer to Aggregators and Imputers for more examples.","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"Let's try hourly, daily, and monthly median as the basis of imputation:","category":"page"},{"location":"man/imputation/#","page":"Imputation","title":"Imputation","text":"hourlyzer = DateValizer(Dict(:dateinterval => Dates.Hour(1)))\nmonthlyzer = DateValizer(Dict(:dateinterval => Dates.Month(1)))\ndailyzer = DateValizer(Dict(:dateinterval => Dates.Day(1)))\n\nfit!(hourlyzer,X)\nhourlyres = transform!(hourlyzer,X)\n\nfit!(dailyzer,X)\ndailyres = transform!(dailyzer,X)\n\nfit!(monthlyzer,X)\nmonthlyres = transform!(monthlyzer,X)","category":"page"},{"location":"lib/decisiontree/#","page":"Decision Tree","title":"Decision Tree","text":"Author = \"Paulito Palmes\"","category":"page"},{"location":"lib/decisiontree/#lib_decisiontree-1","page":"Decision Tree","title":"DecisionTreeLearners","text":"","category":"section"},{"location":"lib/decisiontree/#","page":"Decision Tree","title":"Decision Tree","text":"Creates an API wrapper for DecisionTrees for pipeline workflow.","category":"page"},{"location":"lib/decisiontree/#Index-1","page":"Decision Tree","title":"Index","text":"","category":"section"},{"location":"lib/decisiontree/#","page":"Decision Tree","title":"Decision Tree","text":"Modules = [TSML.DecisionTreeLearners]","category":"page"},{"location":"lib/decisiontree/#AutoDocs-1","page":"Decision Tree","title":"AutoDocs","text":"","category":"section"},{"location":"lib/decisiontree/#","page":"Decision Tree","title":"Decision Tree","text":"Modules = [TSML.DecisionTreeLearners]","category":"page"},{"location":"lib/decisiontree/#TSML.DecisionTreeLearners.Adaboost","page":"Decision Tree","title":"TSML.DecisionTreeLearners.Adaboost","text":"Adaboost(\n  Dict(\n    :output => :class,\n    :num_iterations => 7\n  )\n)\n\nAdaboosted decision tree stumps. See DecisionTree.jl's documentation\n\nHyperparameters:\n\n:num_iterations => 7 (number of iterations of AdaBoost)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/decisiontree/#TSML.DecisionTreeLearners.PrunedTree","page":"Decision Tree","title":"TSML.DecisionTreeLearners.PrunedTree","text":"PrunedTree(\n  Dict(\n    :purity_threshold => 1.0,\n    :max_depth => -1,\n    :min_samples_leaf => 1,\n    :min_samples_split => 2,\n    :min_purity_increase => 0.0\n  )\n)\n\nDecision tree classifier.   See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:purity_threshold => 1.0 (merge leaves having >=thresh combined purity)\n:max_depth => -1 (maximum depth of the decision tree)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/decisiontree/#TSML.DecisionTreeLearners.RandomForest","page":"Decision Tree","title":"TSML.DecisionTreeLearners.RandomForest","text":"RandomForest(\n  Dict(\n    :output => :class,\n    :num_subfeatures => 0,\n    :num_trees => 10,\n    :partial_sampling => 0.7,\n    :max_depth => -1\n  )\n)\n\nRandom forest classification.  See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:num_subfeatures => 0  (number of features to consider at random per split)\n:num_trees => 10 (number of trees to train)\n:partial_sampling => 0.7 (fraction of samples to train each tree on)\n:max_depth => -1 (maximum depth of the decision trees)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/decisiontree/#TSML.TSMLTypes.fit!-Union{Tuple{T}, Tuple{Adaboost,T,Array{T,1} where T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.fit!","text":"fit!(adaboost::Adaboost, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to optimize the hyperparameters of Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/decisiontree/#TSML.TSMLTypes.fit!-Union{Tuple{T}, Tuple{PrunedTree,T,Array{T,1} where T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.fit!","text":"fit!(tree::PrunedTree, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to optimize the hyperparameters of PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/decisiontree/#TSML.TSMLTypes.fit!-Union{Tuple{T}, Tuple{RandomForest,T,Array{T,1} where T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.fit!","text":"fit!(forest::RandomForest, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to optimize the parameters of the RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/decisiontree/#TSML.TSMLTypes.transform!-Union{Tuple{T}, Tuple{Adaboost,T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.transform!","text":"transform!(adaboost::Adaboost, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to predict using the optimized hyperparameters of the trained Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/decisiontree/#TSML.TSMLTypes.transform!-Union{Tuple{T}, Tuple{PrunedTree,T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.transform!","text":"transform!(tree::PrundTree, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to predict using the optimized hyperparameters of the trained PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/decisiontree/#TSML.TSMLTypes.transform!-Union{Tuple{T}, Tuple{RandomForest,T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Decision Tree","title":"TSML.TSMLTypes.transform!","text":"transform!(forest::RandomForest, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nFunction to predict using the optimized hyperparameters of the trained RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#","page":"Types and Functions","title":"Types and Functions","text":"Author = \"Paulito Palmes\"","category":"page"},{"location":"lib/functions/#lib_functions-1","page":"Types and Functions","title":"Types and Functions","text":"","category":"section"},{"location":"lib/functions/#","page":"Types and Functions","title":"Types and Functions","text":"Creates an API wrapper for DecisionTrees for pipeline workflow.","category":"page"},{"location":"lib/functions/#Index-1","page":"Types and Functions","title":"Index","text":"","category":"section"},{"location":"lib/functions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [TSML.Outliernicers, TSML.Plotters]","category":"page"},{"location":"lib/functions/#AutoDocs-1","page":"Types and Functions","title":"AutoDocs","text":"","category":"section"},{"location":"lib/functions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [TSML.Outliernicers, TSML.Plotters]","category":"page"},{"location":"lib/functions/#TSML.Outliernicers.Outliernicer","page":"Types and Functions","title":"TSML.Outliernicers.Outliernicer","text":"Outliernicer(Dict())\n\nDetects outliers below or above (q25-iqr,q75+iqr) and replace them with missing so that ValNNer can use nearest neighbors to replace the missings.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.Plotters.Plotter","page":"Types and Functions","title":"TSML.Plotters.Plotter","text":"Plotter()\n\nPlots a TS by default but performs interactive plotting if specified during instance creation.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.TSMLTypes.transform!-Union{Tuple{T}, Tuple{Plotter,T}} where T<:Union{DataFrame, Array{T,1} where T, Array{T,2} where T}","page":"Types and Functions","title":"TSML.TSMLTypes.transform!","text":"Convert missing into NaN for plotting discontinuity\n\n\n\n\n\n","category":"method"}]
}
